{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2109a08e-2809-4b37-8e16-4c5f935514a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vidhi Dhakan\\\\OneDrive\\\\Documents\\\\tutor resume_files\\\\spark-3.5.1'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3076f484-5826-41d9-8497-ff3dfdde85b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=application, master=local) created by __init__ at C:\\Users\\Vidhi Dhakan\\AppData\\Local\\Temp\\ipykernel_9436\\1256454220.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m conf \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39mSparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mpyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\python\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\python\\pyspark\\context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    454\u001b[0m             currentAppName,\n\u001b[0;32m    455\u001b[0m             currentMaster,\n\u001b[0;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[0;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[0;32m    459\u001b[0m         )\n\u001b[0;32m    460\u001b[0m     )\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=application, master=local) created by __init__ at C:\\Users\\Vidhi Dhakan\\AppData\\Local\\Temp\\ipykernel_9436\\1256454220.py:4 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('application').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "73cc1e3f-03c5-4443-837f-45cc0dd2343c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.189:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22b6da13890>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "913bcb23-b7c2-4561-aa16-bb5f6a9c2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b72b9a48-1c28-43b0-a590-de7263705bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as py \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "000a36f4-e328-4956-b02f-e8c438e31d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.read_csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05f94fdd-3019-4273-9c31-4fd336e792e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3557098-ba4c-441b-ae70-9aefe34bd455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cc8e7b5-4539-48f3-b830-bf51c963c172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+----+\n",
      "|    _c0|         _c1|   _c2| _c3|\n",
      "+-------+------------+------+----+\n",
      "|   Name|  Department|Salary|NULL|\n",
      "|   siya|data science| 20000|NULL|\n",
      "|  somya|    IT field| 30000|NULL|\n",
      "|  mehul|   big data | 45000|NULL|\n",
      "|  priya|   big data | 15000|NULL|\n",
      "| piyush|data science| 18000|NULL|\n",
      "|krishna|         IOT| 19000|NULL|\n",
      "|  shona|   big data | 40000|NULL|\n",
      "|  vidhi|data science| 60000|    |\n",
      "|   NULL|            |  NULL|NULL|\n",
      "+-------+------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "171e3d4a-8fa6-4c4b-8c31-8d8c9814fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "007a088f-5b45-42cb-9320-a66c6832f2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b8c65-f5cf-4263-91af-27aaeecc4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################              CHAPTER 2         ######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "24e758e5-3a99-49ce-8c50-5658d2f1374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  pyspark dataframe\n",
    "##  read the dataset\n",
    "## check datatypes of the coloumn(schema)\n",
    "## selecting coloumns and indexing\n",
    "##  check describe options similar to pandas\n",
    "## adding coloumns \n",
    "## dropping coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c877da0-a19d-4cd5-b584-6f41704e9dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##read the dataset\n",
    "\n",
    "df_pyspark=spark.read.option('header','true').csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31aa199e-48fd-464e-a2b6-731e24e96d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check the schema \n",
    "\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8587b42-9ad0-4ec5-9d5f-e3550f06701c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3830ccf3-161e-4b42-bb22-6fdebbdbfbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Department', 'Salary', '_c3']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##selecting coloumns and indecing\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24097b4e-02e9-490b-a8f5-cc23ae8e5852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='siya', Department='data science', Salary=20000, _c3=None),\n",
       " Row(Name='somya', Department='IT field', Salary=30000, _c3=None),\n",
       " Row(Name='mehul', Department='big data ', Salary=45000, _c3=None)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## select coloumn index\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40b384f3-3a72-4283-9146-c5894cf381bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+----+\n",
      "|   Name|  Department|Salary| _c3|\n",
      "+-------+------------+------+----+\n",
      "|   siya|data science| 20000|NULL|\n",
      "|  somya|    IT field| 30000|NULL|\n",
      "|  mehul|   big data | 45000|NULL|\n",
      "|  priya|   big data | 15000|NULL|\n",
      "| piyush|data science| 18000|NULL|\n",
      "|krishna|         IOT| 19000|NULL|\n",
      "|  shona|   big data | 40000|NULL|\n",
      "|  vidhi|data science| 60000|    |\n",
      "|   NULL|            |  NULL|NULL|\n",
      "+-------+------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "396fa154-d91c-4127-b37a-53936d520766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce521bbe-d4ea-46cd-b39e-9d3789bdcef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `experience` cannot be resolved. Did you mean one of the following? [`Department`, `Name`, `Salary`, `_c3`].;\n'Project [Name#286, 'experience]\n+- Relation [Name#286,Department#287,Salary#288,_c3#289] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_pyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexperience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\python\\pyspark\\sql\\dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3184\u001b[0m \n\u001b[0;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `experience` cannot be resolved. Did you mean one of the following? [`Department`, `Name`, `Salary`, `_c3`].;\n'Project [Name#286, 'experience]\n+- Relation [Name#286,Department#287,Salary#288,_c3#289] csv\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name','experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28f9d5-c361-433b-aa6c-72ed6560b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "e4216582-17d5-4f66-8821-79d637fbe92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age ', 'int'), ('experience', 'int')]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check datatypes\n",
    "\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "6b709c10-c7c1-47c0-a325-48d084d92405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+------------------+\n",
      "|summary| Name|             Age |        experience|\n",
      "+-------+-----+-----------------+------------------+\n",
      "|  count|    4|                4|                 4|\n",
      "|   mean| NULL|             27.5|               2.5|\n",
      "| stddev| NULL|6.454972243679028|1.2909944487358056|\n",
      "|    min|Vidhi|               22|                 1|\n",
      "|    max|sunny|               36|                 4|\n",
      "+-------+-----+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### check options \n",
    "\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "00767bc1-7104-462c-a735-ede6dcbf233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------------------------+\n",
      "| Name|Age |experience|experience after 2 years|\n",
      "+-----+----+----------+------------------------+\n",
      "|Vidhi|  29|         2|                       2|\n",
      "|priya|  36|         3|                       3|\n",
      "|sunny|  22|         4|                       4|\n",
      "|shona|  23|         1|                       1|\n",
      "+-----+----+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### adding coloums \n",
    "\n",
    "df_pyspark=df_pyspark.withColumn('experience after 2 years',df_pyspark['experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "b74ca9af-e8d6-4fcc-9afb-8d602ad2a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+\n",
      "| Name|Age |experience|\n",
      "+-----+----+----------+\n",
      "|Vidhi|  29|         2|\n",
      "|priya|  36|         3|\n",
      "|sunny|  22|         4|\n",
      "|shona|  23|         1|\n",
      "+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### drop columns\n",
    "\n",
    "df_pyspark.drop('experience after 2 years').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "3cc2499a-314b-4a2f-bd33-4c1040fe0587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|New Name|Age |experience|\n",
      "+--------+----+----------+\n",
      "|   Vidhi|  29|         2|\n",
      "|   priya|  36|         3|\n",
      "|   sunny|  22|         4|\n",
      "|   shona|  23|         1|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### rename columns \n",
    "\n",
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51d3c7-ae5f-4f67-8b44-9b5d9d4103d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "################                CHAPTER  3   ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c441a07-0a85-4018-bb3e-c415830a8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING MODULE \n",
    "\n",
    "# HOW TO HANDLE MISSING VALUES IN PYSPARK \n",
    "\n",
    "1. DROPPING COLUMNS \n",
    "2. DROPPING ROW \n",
    "3. VARIOUS PARAMETERS IN DROPPING FUNCTIONALITY\n",
    "4. HANDLING MISSING VALUES BY MEAN . MEDIAN, MODE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6fdd4dd-b953-488c-8ec7-8a9f96abbb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|   _c0| _c1|       _c2|    _c3|\n",
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "|  mahi|  30|         2|  90000|\n",
      "|      |NULL|      NULL|       |\n",
      "|      |NULL|          |       |\n",
      "|  NULL|    |      NULL|   NULL|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "962c8a1d-83e4-4995-9d0e-22ffec31a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "dcb63134-5d9c-4633-a76b-12f66fa1313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "+------+----+----------+-------+\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "|  mahi|NULL|         2|  90000|\n",
      "|  ravi|NULL|      NULL|  30000|\n",
      "| sonal|  22|        10|  60000|\n",
      "|  NULL|  34|      NULL|   NULL|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "11858ee2-9ef9-4ca2-b729-b9d4cbbaa80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+\n",
      "|Age |experience|Salary |\n",
      "+----+----------+-------+\n",
      "|  29|         2|  20000|\n",
      "|  36|         3| 300000|\n",
      "|  22|         4|   1500|\n",
      "|  23|         1|  20000|\n",
      "|  23|         2|  80000|\n",
      "|NULL|         2|  90000|\n",
      "|NULL|      NULL|  30000|\n",
      "|  22|        10|  60000|\n",
      "|  34|      NULL|   NULL|\n",
      "+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop the coloumns\n",
    "\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "bb1d3d41-8f09-4a04-a397-b88098835426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "+------+----+----------+-------+\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "|  mahi|NULL|         2|  90000|\n",
      "|  ravi|NULL|      NULL|  30000|\n",
      "| sonal|  22|        10|  60000|\n",
      "|  NULL|  34|      NULL|   NULL|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1735636d-80bb-4e98-ad98-ab16e60692dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "+------+----+----------+-------+\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "| sonal|  22|        10|  60000|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop the null values \n",
    "\n",
    "df_pyspark.na.drop().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "aebe7ec3-3049-4ee6-8eb5-279bb64e6dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "+------+----+----------+-------+\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "| sonal|  22|        10|  60000|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop the null values ----------------- HOW === ANY \n",
    "\n",
    "df_pyspark.na.drop('any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "b93a7526-a5ec-42ff-8b3b-a78f0101265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "+------+----+----------+-------+\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "|  mahi|NULL|         2|  90000|\n",
      "|  ravi|NULL|      NULL|  30000|\n",
      "| sonal|  22|        10|  60000|\n",
      "|  NULL|  34|      NULL|   NULL|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop('all').show() -------------------- if completely null than only it works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b1233e4f-1284-4669-ac3a-3d1e6656ac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "+------+----+----------+-------+\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "|  mahi|NULL|         2|  90000|\n",
      "| sonal|  22|        10|  60000|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop null values ------- threshold = 2 ============== it removes non null values \n",
    "\n",
    "df_pyspark.na.drop('any',thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "68f8977e-dd3a-4cc6-be59-fa7e7e015e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "+------+----+----------+-------+\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "|  mahi|NULL|         2|  90000|\n",
      "| sonal|  22|        10|  60000|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### subset -------- specific null values will remove \n",
    "\n",
    "df_pyspark.na.drop(how='any',subset=['experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "ffb22cf6-b22b-4f5f-88a8-3b17f6965aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "+------+----+----------+-------+\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "|  mahi|NULL|         2|  90000|\n",
      "|  ravi|NULL|      NULL|  30000|\n",
      "| sonal|  22|        10|  60000|\n",
      "|  NULL|  34|      NULL|   NULL|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "2d769831-9195-4289-8aae-a450b131ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------------+-------+\n",
      "|  Name|Age |    experience|Salary |\n",
      "+------+----+--------------+-------+\n",
      "| Vidhi|  29|             2|  20000|\n",
      "| priya|  36|             3| 300000|\n",
      "| sunny|  22|             4|   1500|\n",
      "| shona|  23|             1|  20000|\n",
      "|piyush|  23|             2|  80000|\n",
      "|  mahi|NULL|             2|  90000|\n",
      "|  ravi|NULL|Missing Values|  30000|\n",
      "| sonal|  22|            10|  60000|\n",
      "|  NULL|  34|Missing Values|   NULL|\n",
      "+------+----+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### -------- filling the missing value s\n",
    "\n",
    "df_pyspark.na.fill('Missing Values', 'experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "f9949d98-12c9-4aa7-aa0c-4d1c287acb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ------------------------------- MEAN, MODE, MEDIAN --------------------------------------------\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['experience'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['experience',]]\n",
    "\t).setStrategy(\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "b187d863-4a98-4cf6-b75d-bb2f1b8fb0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+------------------+\n",
      "|  Name|Age |experience|Salary |experience_imputed|\n",
      "+------+----+----------+-------+------------------+\n",
      "| Vidhi|  29|         2|  20000|                 2|\n",
      "| priya|  36|         3| 300000|                 3|\n",
      "| sunny|  22|         4|   1500|                 4|\n",
      "| shona|  23|         1|  20000|                 1|\n",
      "|piyush|  23|         2|  80000|                 2|\n",
      "|  mahi|NULL|         2|  90000|                 2|\n",
      "|  ravi|NULL|      NULL|  30000|                 3|\n",
      "| sonal|  22|        10|  60000|                10|\n",
      "|  NULL|  34|      NULL|   NULL|                 3|\n",
      "+------+----+----------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac79b5-5795-4fe6-acab-65e743ec11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------CHAPTER 4 - FILTER OPERATIONS ------------------------------------------------\n",
    "\n",
    "1. FILTER OPERATION\n",
    "2. &,|,==\n",
    "3. ~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17732c-1fa8-46c4-b3ee-4bf30f6c792e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "9eca2840-e50c-4a9f-940b-1268f0044336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+-------+\n",
      "|  Name|Age |experience|Salary |\n",
      "+------+----+----------+-------+\n",
      "| Vidhi|  29|         2|  20000|\n",
      "| priya|  36|         3| 300000|\n",
      "| sunny|  22|         4|   1500|\n",
      "| shona|  23|         1|  20000|\n",
      "|piyush|  23|         2|  80000|\n",
      "|  mahi|  30|         2|  90000|\n",
      "|      |NULL|      NULL|       |\n",
      "|      |NULL|          |       |\n",
      "|  NULL|    |      NULL|   NULL|\n",
      "+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55080ec8-edd1-4934-8855-168cc75470bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vidhi Dhakan\\\\OneDrive\\\\Documents\\\\tutor resume_files\\\\spark-3.5.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4987b4b0-ca2d-4a6f-857b-0aeec7542844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('application').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1282d7-4dba-455c-be29-89c0b4729263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.189:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x260fa18b620>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d341b6-4735-486f-af89-8254faf6973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e596784-6332-4860-9199-aa2e41afec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as py \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31734bbc-3adc-4871-b6ed-5d1d8b4e9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pyspark=spark.read.option('header','true').csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36bc165-e9f5-4c20-b278-d3f0ba0dea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pyspark=spark.read.option('header','true').csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba74aa2-504f-453e-bf65-d965d8943252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Vidhi| 29|\n",
      "|sunny| 22|\n",
      "|shona| 23|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Trim the column names\n",
    "trimmed_df = df_pyspark.select([col(c).alias(c.strip()) for c in df_pyspark.columns])\n",
    "\n",
    "# Now you can filter and select as intended\n",
    "trimmed_df.filter(\"Salary <= 20000\").select('Name', 'Age').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933e55a-471d-4ac2-b434-6dbbac236faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"ColumnTrimming\").getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_pyspark = spark.read.option('header', 'true').csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv', inferSchema=True)\n",
    "\n",
    "# Print the column names to see if there are any leading/trailing spaces\n",
    "print(\"Original column names:\", df_pyspark.columns)\n",
    "\n",
    "# Trim the column names and rename them\n",
    "trimmed_df = df_pyspark.select([col(c).alias(c.strip()) for c in df_pyspark.columns])\n",
    "\n",
    "# Print the new column names to verify\n",
    "print(\"Trimmed column names:\", trimmed_df.columns)\n",
    "\n",
    "# Filter and select the DataFrame\n",
    "trimmed_df.filter(\"Salary <= 20000\").select('Name', 'Age').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6f4264e-3280-4dab-9235-df20cffdabf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Vidhi| 29|         2| 20000|\n",
      "|sunny| 22|         4|  1500|\n",
      "|shona| 23|         1| 20000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##single coloumns\n",
    "\n",
    "filtered_df = trimmed_df.filter((col(\"Salary\") <= 20000)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e13126ac-f0fe-4717-8a18-b760cbf37af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Vidhi| 29|         2| 20000|\n",
      "|shona| 23|         1| 20000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple conditions\n",
    "filtered_df = trimmed_df.filter((col(\"Salary\") <= 20000) & (col(\"Salary\") >= 15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83a18dc7-2bd4-4c07-b83c-7d6b610df4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|experience|Salary|\n",
      "+------+---+----------+------+\n",
      "| priya| 36|         3|300000|\n",
      "|piyush| 23|         2| 80000|\n",
      "|  mahi| 30|         2| 90000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## if i want to use inverse funcitons === ~\n",
    "\n",
    "filtered_df = trimmed_df.filter(~(col(\"Salary\") <= 20000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde8790-f4a0-4676-b660-7b558dd79138",
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------------------------------------------CHAPTER 5 - GROUPBY/ AGGREGATE FUNCTIONS -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fdfa79b-aff4-492e-b66d-25445843dd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vidhi Dhakan\\\\OneDrive\\\\Documents\\\\tutor resume_files\\\\spark-3.5.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64845720-c7c0-47e9-8a78-8e816f0f790e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=application, master=local) created by __init__ at C:\\Users\\Vidhi Dhakan\\AppData\\Local\\Temp\\ipykernel_10452\\1256454220.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m conf \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39mSparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mpyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\python\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\python\\pyspark\\context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    454\u001b[0m             currentAppName,\n\u001b[0;32m    455\u001b[0m             currentMaster,\n\u001b[0;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[0;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[0;32m    459\u001b[0m         )\n\u001b[0;32m    460\u001b[0m     )\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=application, master=local) created by __init__ at C:\\Users\\Vidhi Dhakan\\AppData\\Local\\Temp\\ipykernel_10452\\1256454220.py:4 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('application').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb611cbe-a357-4d1d-8fa1-f1f0dd496b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.189:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x155aea150a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df79614c-b30b-4d1e-aa2f-791a04f92dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d33053c-5990-41a7-8fe0-3cd964bf89f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as py \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "166855fd-a459-4634-acb8-e95239b824f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_pyspark=spark.read.option('header','true').csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16f3c623-aacc-446e-bb3e-8df2fbf151bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv(r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58a08e44-0ad3-4f37-a35e-cfc41bfd0d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+----+\n",
      "|   Name|  Department|Salary| _c3|\n",
      "+-------+------------+------+----+\n",
      "|   siya|data science| 20000|NULL|\n",
      "|  somya|    IT field| 30000|NULL|\n",
      "|  mehul|   big data | 45000|NULL|\n",
      "|  priya|   big data | 15000|NULL|\n",
      "| piyush|data science| 18000|NULL|\n",
      "|krishna|         IOT| 19000|NULL|\n",
      "|  shona|   big data | 40000|NULL|\n",
      "|  vidhi|data science| 60000|    |\n",
      "|   NULL|            |  NULL|NULL|\n",
      "+-------+------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "250fc708-5663-46a3-ada6-625971b82981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "404919f2-c25c-40dc-a58a-9493bdaac0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------+---+\n",
      "| Name|  Department|Salary|_c3|\n",
      "+-----+------------+------+---+\n",
      "|vidhi|data science| 60000|   |\n",
      "+-----+------------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=df_pyspark.na.drop().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66f29912-5c88-4c37-9150-2b73c72a7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################               GROUP BY  ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56fa3a8f-e845-4a17-93c6-f93673f4eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "| piyush|\n",
      "|  vidhi|\n",
      "|  priya|\n",
      "|  somya|\n",
      "|  mehul|\n",
      "|   siya|\n",
      "|krishna|\n",
      "|  shona|\n",
      "|   NULL|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1859e959-38c8-48bc-9d41-3e121bd11216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54381814-e506-4258-a429-c58565262893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+----+\n",
      "|   Name|  Department|Salary| _c3|\n",
      "+-------+------------+------+----+\n",
      "|   siya|data science| 20000|NULL|\n",
      "|  somya|    IT field| 30000|NULL|\n",
      "|  mehul|   big data | 45000|NULL|\n",
      "|  priya|   big data | 15000|NULL|\n",
      "| piyush|data science| 18000|NULL|\n",
      "|krishna|         IOT| 19000|NULL|\n",
      "|  shona|   big data | 40000|NULL|\n",
      "|  vidhi|data science| 60000|    |\n",
      "|   NULL|            |  NULL|NULL|\n",
      "+-------+------------+------+----+\n",
      "\n",
      "Original column names: ['Name', 'Department', 'Salary', '_c3']\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "file_path = r'C:\\Users\\Vidhi Dhakan\\OneDrive\\Documents\\tutor resume_files\\spark-3.5.1\\test 1.csv'\n",
    "df_pyspark = spark.read.option('header', 'true').csv(file_path, inferSchema=True)\n",
    "\n",
    "# Check if the DataFrame is created\n",
    "if df_pyspark is None:\n",
    "    print(\"Failed to create DataFrame. Check file path and format.\")\n",
    "else:\n",
    "    df_pyspark.show()\n",
    "    print(\"Original column names:\", df_pyspark.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43c334d0-92f0-4630-b098-f17e7acc9dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed column names: ['Name', 'Department', 'Salary', '_c3']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Proceed only if df_pyspark is not None\n",
    "if df_pyspark is not None:\n",
    "    # Trim the column names and rename them\n",
    "    trimmed_df = df_pyspark.select([col(c).alias(c.strip()) for c in df_pyspark.columns])\n",
    "\n",
    "    # Print the new column names to verify\n",
    "    print(\"Trimmed column names:\", trimmed_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8db5bea3-1744-4a4f-9003-e5774aa78d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+----+\n",
      "|   Name|  Department|Salary| _c3|\n",
      "+-------+------------+------+----+\n",
      "|   siya|data science| 20000|NULL|\n",
      "|  somya|    IT field| 30000|NULL|\n",
      "|  mehul|   big data | 45000|NULL|\n",
      "|  priya|   big data | 15000|NULL|\n",
      "| piyush|data science| 18000|NULL|\n",
      "|krishna|         IOT| 19000|NULL|\n",
      "|  shona|   big data | 40000|NULL|\n",
      "|  vidhi|data science| 60000|    |\n",
      "|   NULL|            |  NULL|NULL|\n",
      "+-------+------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Proceed only if trimmed_df is not None\n",
    "if 'trimmed_df' in locals():\n",
    "    # Cast Salary column to integer\n",
    "    trimmed_df = trimmed_df.withColumn(\"Salary\", col(\"Salary\").cast(\"int\"))\n",
    "\n",
    "    # Show the DataFrame to verify the cast\n",
    "    trimmed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6090b568-a224-40f2-b774-ce6fc6349baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b2b4dfa-bcfc-4bbb-9d16-61b9776af5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   Name|sum(Salary)|\n",
      "+-------+-----------+\n",
      "|   NULL|       NULL|\n",
      "| piyush|      18000|\n",
      "|  vidhi|      60000|\n",
      "|  priya|      15000|\n",
      "|  somya|      30000|\n",
      "|  mehul|      45000|\n",
      "|   siya|      20000|\n",
      "|krishna|      19000|\n",
      "|  shona|      40000|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## grouped to find maximum salary \n",
    "\n",
    "df_pyspark.groupBy('Name').sum().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18b0e09c-1a2a-4606-a13a-16ad4dca8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      19000|\n",
      "|    IT field|      30000|\n",
      "|data science|      98000|\n",
      "|   big data |     100000|\n",
      "|            |       NULL|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### group by dept to find max salary \n",
    "\n",
    "df_pyspark.groupBy('department').sum().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77a52357-c941-40be-92c3-769df922f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|  department|       avg(Salary)|\n",
      "+------------+------------------+\n",
      "|         IOT|           19000.0|\n",
      "|    IT field|           30000.0|\n",
      "|data science|32666.666666666668|\n",
      "|   big data |33333.333333333336|\n",
      "|            |              NULL|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('department').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6149c83b-4de8-4abe-8eda-98c38af48431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|  department|count|\n",
      "+------------+-----+\n",
      "|         IOT|    1|\n",
      "|    IT field|    1|\n",
      "|data science|    3|\n",
      "|   big data |    3|\n",
      "|            |    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "faa76354-2ef6-43a3-adaf-7dcae82c63ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|max(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      19000|\n",
      "|    IT field|      30000|\n",
      "|data science|      60000|\n",
      "|   big data |      45000|\n",
      "|            |       NULL|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('department').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a16bf7c0-279b-4c73-9abe-be9330a7c399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  department|min(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      19000|\n",
      "|    IT field|      30000|\n",
      "|data science|      18000|\n",
      "|   big data |      15000|\n",
      "|            |       NULL|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('department').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3a9e95a-0e98-4428-adc3-6305a08e1a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|  department|       avg(Salary)|\n",
      "+------------+------------------+\n",
      "|         IOT|           19000.0|\n",
      "|    IT field|           30000.0|\n",
      "|data science|32666.666666666668|\n",
      "|   big data |33333.333333333336|\n",
      "|            |              NULL|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('department').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29c12a-3b26-41e5-b81a-2e79dbac3b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


---------------------------------------------------
